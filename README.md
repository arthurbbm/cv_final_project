# cv\_final\_project

This repository implements a simple pipeline for plantâ€“soil segmentation using simulated datasets generated in PyBullet and a deepâ€learning segmentation model. It also supports applying a styleâ€transfer step (via an external repository) to improve domain adaptation.

---

## ğŸ“‚ Directory Structure

```
.
â”œâ”€â”€ config.yml
â”œâ”€â”€ environment.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pybullet
â”‚   â”œâ”€â”€ generate_train_dataset.py
â”‚   â””â”€â”€ generate_test_dataset.py
â””â”€â”€ segmentation
    â”œâ”€â”€ train.py
    â”œâ”€â”€ inference.py
    â”œâ”€â”€ metrics.py
    â””â”€â”€ visualization.py
```

You will also add your raw inputs here:

```
data/
â””â”€â”€ input/
    â”œâ”€â”€ soil/      # .jpg images of soil textures
    â””â”€â”€ plant/     # .stl mesh files of plant models
```

---

## ğŸ”§ Installation

1. **Clone the repo**

   ```bash
   git clone https://github.com/arthurbbm/cv_final_project
   cd cv_final_project
   ```

2. **Create the conda environment**

   ```bash
   conda env create -f environment.yml
   conda activate cv_final_project
   ```

3. (Optional) You can also install via `pip` if you prefer:

   ```bash
   pip install -r requirements.txt
   ```

---

## âš™ï¸ Configuration

All configurable paths go into `config.yml`.  At minimum, set:

```yaml
pybullet_dataset: /absolute/path/to/data
segmentation_dataset: /path/to/segmentation_dataset
```

* **`pybullet_dataset`**
  Root of your data directory for PyBullet jobs.

  * Input soil images in `â€¦/data/input/soil/`
  * Input plant meshes in `â€¦/data/input/plant/`
  * After running PyBullet scripts, outputs will be written to:

    ```
    [pybullet_dataset]/output/images  
    [pybullet_dataset]/output/masks
    ```
  * **Important:** after each run of `generate_train_dataset.py` or `generate_test_dataset.py`, **rename** the `output/` folder (e.g. `output_train/`, `output_test/`) to prevent overwriting/mixture.

* **`segmentation_dataset`**
  Root of your segmentation dataset.  It should contain two subdirectories:

  ```
  /path/to/segmentation_dataset/
  â”œâ”€â”€ train_sim/           # ground-truth images/masks
  â””â”€â”€ inference_train_sim/ # predicted masks
  ```

---

## ğŸ¤– Dataset Generation with PyBullet

**What is PyBullet?**
PyBullet is a physicsâ€simulation library that can simulate rigidâ€body dynamics and sensor rendering. Here itâ€™s used to spawn synthetic scenes of soil + plant meshes and render:

* **RGB images** â†’ saved to `â€¦/output/images`
* **Groundâ€truth masks** â†’ saved to `â€¦/output/masks`

### Usage

```bash
# Generate training dataset
python pybullet/generate_train_dataset.py

# Generate testing dataset
python pybullet/generate_test_dataset.py
```

After each run, rename the `â€¦/output/` directory before running the other script.

---

## ğŸ¨ (Optional) Styleâ€‘Transfer

This project does **not** implement a new styleâ€‘transfer method, but you can apply one to make your simulated images look more â€œreal.â€ We recommend the DTâ€‘MARSâ€‘CycleGAN codebase:

> [https://github.com/UGA-BSAIL/DT-MARS-CycleGAN](https://github.com/UGA-BSAIL/DT-MARS-CycleGAN)

You can use their pretrained models or train your own, then apply to the images in your PyBullet `output/images` before training the segmentation network.

---

## ğŸ‹ï¸â€â™€ï¸ Training the Segmentation Model

The segmentation scripts live under `segmentation/`.  First, make sure your combined dataset (possibly styleâ€‘transferred) is arranged as:

```
/path/to/dataset/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ img_0001.png
â”‚   â””â”€â”€ â€¦
â””â”€â”€ masks/
    â”œâ”€â”€ mask_0001.png
    â””â”€â”€ â€¦
```

### Command

```bash
cd segmentation
python train.py \
  --n_epochs 10 \
  --batchSize 4 \
  --dataroot /path/to/dataset \
  --outdir /path/to/save/model \
  --checkpoint_freq 5 \
  --lr 1e-4 \
  --num_labels 2 \
  [--device cuda]
```

* **Train script**: `train.py` is the correct entry point.
* **File naming requirement**: The script pairs images and masks by sorted orderâ€”make sure each RGB image and its corresponding mask share the same filename (aside from extension) so that after sorting they align correctly.
* **Output files** (in `--outdir`):

  * `checkpoint.pth` (every `checkpoint_freq` epochs)
  * `segmentation_model.pth` (final `state_dict`)

---

## ğŸ” Running Inference

Once you have a trained model (`.pth`), you can predict on new images:

```bash
cd segmentation
python inference.py \
  --model_path /path/to/segmentation_model.pth \
  --num_labels 2 \
  --test_dir /path/to/test/images \
  --output_dir /path/to/save/predicted/masks \
  [--device cuda]
```

Predicted masks will be saved as PNGs in `output_dir`.

---

## ğŸ“ Metrics & Visualization

This project includes scripts for evaluating and visualizing model predictions.

### 1. Metrics (`metrics.py`)

* **Purpose:** Compute image-wise and overall segmentation metrics (accuracy, precision, recall, F1-score, IoU).
* **Configuration:** Ensure `config.yml` contains:

  ```yaml
  segmentation_dataset: /path/to/segmentation_dataset
  ```

  with structure:

  ```
  segmentation_dataset/
  â”œâ”€â”€ train_sim/             # ground-truth images and masks
  â””â”€â”€ inference_train_sim/   # predicted masks (same filenames)
  ```
* **Usage:**

  ```bash
  cd segmentation
  python metrics.py
  ```
* **Outputs:**

  * `metrics_imagewise.csv` â€“ per-image metrics table
  * `metrics_overall.csv`   â€“ aggregated metrics summary

### 2. Visualization (`visualization.py`)

* **Purpose:** Overlay predicted masks on original RGB images and save blended figures for qualitative inspection.
* **Configuration:** Uses the same `segmentation_dataset` path from above.
* **Usage:**

  ```bash
  cd segmentation
  python visualization.py
  ```
